import asyncio
import time
from datetime import datetime, timezone
from typing import Any, Mapping

from loguru import logger
from motor.motor_asyncio import AsyncIOMotorDatabase
from pymongo import UpdateOne

from modules.bitcoin_rpc import BitcoinRPC
from modules.msg_dispatcher import Message
from modules.tx_enricher import TxEnricher


class Indexer:
    """Handles writing data to MongoDB from the DB queue with logging."""

    def __init__(
        self,
        rpc: BitcoinRPC,
        db: AsyncIOMotorDatabase[Mapping[str, Any]],
        db_queue: asyncio.Queue[Message],
    ):
        self.rpc = rpc
        self.db = db
        self.tx_enricher = TxEnricher(rpc, db)
        self.db_queue = db_queue

        self.process_functions = {
            "blocks": self.process_block,
            "transactions": self.process_transaction,
            "peers": self.process_peers,
        }

    async def run(self):
        logger.info("DB Indexer started...")
        while True:
            try:
                start_time = time.time_ns()
                msg: Message = await self.db_queue.get()

                process_function = self.process_functions.get(msg.queue_id)
                if not process_function:
                    logger.error(f"‚ùå Invalid queue ID: {msg.queue_id}")
                    continue

                await process_function(msg.payload)
                duration = (time.time_ns() - start_time) / 1e6
                logger.debug(
                    f"üü¢ Processed db entry for {msg.queue_id} in {duration:.2f}ms"
                )
                self.db_queue.task_done()
            except asyncio.CancelledError:
                logger.info("Indexer cancelled.")
                raise
            except Exception as e:
                logger.error(f"‚ùå Indexer Error: {e}")

    async def process_block(self, block_data: dict):
        """Process a block and its transactions with bulk operations."""
        try:
            # Update block document
            await self.db.blocks.update_one(
                {"hash": block_data["hash"]},
                {"$set": block_data},
                upsert=True,
            )

            # Process balances with bulk operations
            balance_updates = []
            for tx in block_data["tx"]:
                enriched_tx = await self.tx_enricher.enrich_transaction(
                    tx,
                    block_time=block_data["time"],
                    block_hash=block_data["hash"],
                    block_height=block_data["height"],
                )

                await self.db.transactions.update_one(
                    {"txid": enriched_tx["txid"]},
                    {"$set": enriched_tx},
                    upsert=True,
                )

                # Invalidate cache for the updated transaction
                await self.tx_enricher.invalidate_previous_transaction_cache(
                    enriched_tx["txid"]
                )

                # Process vouts for balance updates
                for vout in enriched_tx.get("vout", []):
                    value = vout.get("value", 0.0)
                    addresses = vout.get("scriptPubKey", {}).get("addresses", [])
                    for address in addresses:
                        balance_updates.append(
                            UpdateOne(
                                {"address": address},
                                {
                                    "$inc": {"balance": value},
                                    "$set": {
                                        "block_time": block_data["time"],
                                        "block_height": block_data["height"],
                                    },
                                },
                                upsert=True,
                            )
                        )

                # Process vins for balance updates
                for vin in enriched_tx.get("vin", []):
                    value = vin.get("value", 0.0)
                    addresses = vin.get("addresses", [])
                    if value == 0.0 and vin.get(
                        "txid"
                    ):  # Bugfix: Check for potential missed vin value
                        logger.warning(
                            f"Potential missing input value for vin in txid={enriched_tx['txid']}, "
                            f"prev_txid={vin.get('txid')},"
                            f" vout_index={vin.get('vout')}. Balance might be incorrect."
                        )
                    for address in addresses:
                        balance_updates.append(
                            UpdateOne(
                                {"address": address},
                                {
                                    "$inc": {"balance": -value},
                                    "$set": {
                                        "block_time": block_data["time"],
                                        "block_height": block_data["height"],
                                    },
                                },
                                upsert=True,
                            )
                        )

            # Execute balance bulk update
            if balance_updates:
                _ = await self.db.balances.bulk_write(balance_updates, ordered=False)
                logger.debug(f"üìä Bulk updated {len(balance_updates)} balances")

            # Update last processed block height
            await self.update_last_processed(
                "blocks", {"last_height": block_data["height"]}
            )

        except Exception as e:
            logger.error(f"Error processing block {block_data['height']}: {e}")
            raise

    async def process_transaction(self, tx_data: dict):
        """Process a single transaction."""
        try:
            enriched_tx = await self.tx_enricher.enrich_transaction(tx_data)
            await self.db.transactions.update_one(
                {"txid": enriched_tx["txid"]},
                {"$set": enriched_tx},
                upsert=True,
            )
            await self.tx_enricher.invalidate_previous_transaction_cache(
                enriched_tx["txid"]
            )
            await self.update_last_processed(
                "transactions", {"last_txid": tx_data["txid"]}
            )
        except Exception as e:
            logger.error(f"Error processing transaction {tx_data['txid']}: {e}")
            raise

    async def process_peers(self, peers: list):
        """Process peer data with bulk operations."""
        try:
            # Clear existing peers
            await self.db.peers.delete_many({})
            # Bulk insert new peers
            await self.db.peers.insert_many(peers)
            await self.update_last_processed(
                "peers", {"last_updated": datetime.now(timezone.utc)}
            )
        except Exception as e:
            logger.error(f"Error processing peers: {e}")
            raise

    async def update_last_processed(self, key: str, value: dict):
        """Updates the last processed document in the system collection."""
        try:
            _ = await self.db.system.update_one(
                {"_id": key}, {"$set": value}, upsert=True
            )
            if key == "blocks" and value["last_height"] % 1000 == 0:
                logger.info(f"‚úÖ Processed block {value}")
        except Exception as e:
            logger.error(f"Error updating system tracker for {key}: {e}")
            raise
